{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db26d897",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tensorboardX==2.5.1 imgaug==0.4.0 scikit-image==0.19.3 albumentations==1.1.0 python-box==7.1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a148a2f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Run LAA-Net inference on folders of images and videos.\n",
        "\n",
        "Images and video frames are first processed through a YuNet-based face\n",
        "alignment pipeline that expands the detected face, applies aspect-ratio\n",
        "preserving letterbox padding, and optionally tracks faces across frames when\n",
        "detections drop out. The resulting square crops reuse the normalization metadata\n",
        "from ``scripts/test.py`` before being forwarded through the network.\n",
        "\n",
        "Helper routines:\n",
        "\n",
        "``prepare_model`` loads the YAML configuration, builds the network, restores\n",
        "weights, and prepares the normalization transforms.\n",
        "\n",
        "``preprocess_image`` detects and standardizes a single RGB image ahead of the\n",
        "torch preprocessing chain.\n",
        "\n",
        "``preprocess_frame`` performs the same detection pipeline on BGR video frames\n",
        "while smoothing boxes over time and falling back to tracking when possible.\n",
        "\n",
        "``run_inference`` enumerates the ``data`` directory, runs preprocessing, saves\n",
        "optional face crops, and reports logits alongside auxiliary face metadata.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from configs.get_config import load_config\n",
        "from losses.losses import _sigmoid\n",
        "from models import MODELS, build_model, load_pretrained\n",
        "from package_utils.face_processing import FacePreprocessor, FacePreprocessingConfig\n",
        "from package_utils.image_utils import load_image\n",
        "from package_utils.transform import final_transform\n",
        "\n",
        "\n",
        "# The SBI checkpoint pairs with the SBI configuration, whose metadata keeps the\n",
        "# preprocessing codepath aligned with ``scripts/test.py``.\n",
        "DEFAULT_CONFIG_PATH = \"configs/efn4_fpn_sbi_adv.yaml\"\n",
        "DEFAULT_WEIGHT_PATH = os.path.join(\n",
        "    \"model\",\n",
        "    \"laa_net\",\n",
        "    \"PoseEfficientNet_EFN_hm10_EFPN_NoBasedCLS_Focal_C3_256Cst100_8SBI_\"\n",
        "    \"SAM(Adam)_ADV_Era1_OutSigmoid_1e7_boost500_UnFZ_model_best.pth\",\n",
        ")\n",
        "DEFAULT_DATA_DIR = \"data\"\n",
        "\n",
        "IMAGE_EXTENSIONS = (\".png\", \".jpg\", \".jpeg\", \".bmp\")\n",
        "VIDEO_EXTENSIONS = (\".mp4\", \".avi\", \".mov\", \".mkv\", \".webm\")\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    \"\"\"Parse command-line arguments for configuring the inference script.\"\"\"\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=(\n",
        "            \"Run SBI single-image inference using the exact preprocessing\"\n",
        "            \" pipeline from scripts/test.py.\"\n",
        "        )\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--config\",\n",
        "        default=DEFAULT_CONFIG_PATH,\n",
        "        help=\"Path to the YAML config that defines the model architecture.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--weights\",\n",
        "        default=DEFAULT_WEIGHT_PATH,\n",
        "        help=(\n",
        "            \"Checkpoint path to load. The default matches the SBI config;\"\n",
        "            \" use the BI config if you swap in the BI weights.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--data-dir\",\n",
        "        default=DEFAULT_DATA_DIR,\n",
        "        help=\"Directory containing RGB images to evaluate.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--skip-fixed-crop\",\n",
        "        action=\"store_true\",\n",
        "        help=(\n",
        "            \"Disable the fixed 257x257 crop after resizing to 317x317. \"\n",
        "            \"This can be useful if the face is heavily off-center.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--video-frame-step\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        help=(\n",
        "            \"Decode one frame out of every N when processing videos. \"\n",
        "            \"Keeping the default of 1 mirrors the evaluation pipeline's \"\n",
        "            \"use of all decoded frames before averaging logits.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--yunet-path\",\n",
        "        default=\"face_detection_yunet_2023mar.onnx\",\n",
        "        help=\"Path to the YuNet ONNX model used for face detection.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--save-preprocessed-images-path\",\n",
        "        default=None,\n",
        "        help=\"Optional directory to store square face crops and metadata.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--verbose\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Print per-file prediction details.\",\n",
        "    )\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def _validate_inference_fields(cfg) -> None:\n",
        "    \"\"\"Ensure the loaded config exposes the metadata required by this demo.\"\"\"\n",
        "\n",
        "    missing_paths = []\n",
        "    dataset_cfg = getattr(cfg, \"DATASET\", None)\n",
        "    if dataset_cfg is None:\n",
        "        missing_paths.append(\"DATASET\")\n",
        "    else:\n",
        "        if not hasattr(dataset_cfg, \"IMAGE_SIZE\"):\n",
        "            missing_paths.append(\"DATASET.IMAGE_SIZE\")\n",
        "        if not hasattr(dataset_cfg, \"TRANSFORM\") or not hasattr(\n",
        "            dataset_cfg.TRANSFORM, \"normalize\"\n",
        "        ):\n",
        "            missing_paths.append(\"DATASET.TRANSFORM.normalize\")\n",
        "\n",
        "    if not hasattr(cfg, \"TEST\") or not hasattr(cfg.TEST, \"threshold\"):\n",
        "        missing_paths.append(\"TEST.threshold\")\n",
        "\n",
        "    if missing_paths:\n",
        "        raise ValueError(\n",
        "            \"The supplied config is missing fields required for preprocessing \"\n",
        "            f\"or scoring: {', '.join(missing_paths)}\"\n",
        "        )\n",
        "\n",
        "    # ``DATASET.DATA`` documents how training/evaluation loaders enumerate\n",
        "    # samples. The ad-hoc ``data/`` directory this demo scans can differ, but\n",
        "    # surfacing unsupported types early helps users diagnose mismatches.\n",
        "    data_cfg = getattr(dataset_cfg, \"DATA\", None)\n",
        "    if data_cfg is not None and hasattr(data_cfg, \"TYPE\"):\n",
        "        supported = {\"frames\", \"images\"}\n",
        "        if data_cfg.TYPE not in supported:\n",
        "            raise ValueError(\n",
        "                \"This demo only supports configs whose DATASET.DATA.TYPE is in \"\n",
        "                f\"{sorted(supported)}. Got '{data_cfg.TYPE}'.\"\n",
        "            )\n",
        "\n",
        "\n",
        "def prepare_model(\n",
        "    cfg_path: str, weight_path: str\n",
        ") -> Tuple[object, torch.nn.Module, object, torch.device, List[int]]:\n",
        "    \"\"\"Assemble the model and preprocessing primitives required for inference.\"\"\"\n",
        "\n",
        "    cfg = load_config(cfg_path)\n",
        "    _validate_inference_fields(cfg)\n",
        "    model = build_model(cfg.MODEL, MODELS).to(torch.float32)\n",
        "    model = load_pretrained(model, weight_path)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    transforms = final_transform(cfg.DATASET)\n",
        "    image_size = cfg.DATASET.IMAGE_SIZE\n",
        "\n",
        "    return cfg, model, transforms, device, image_size\n",
        "\n",
        "\n",
        "def _prepare_tensor(\n",
        "    image_rgb: np.ndarray, transforms, device: torch.device\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Normalize a preprocessed RGB image and move it onto the target device.\"\"\"\n",
        "\n",
        "    tensor = transforms(image_rgb.astype(np.float32) / 255.0)\n",
        "    tensor = tensor.unsqueeze(0).to(device=device, dtype=torch.float32)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def preprocess_image(\n",
        "    image_path: str,\n",
        "    face_processor: FacePreprocessor,\n",
        "    transforms,\n",
        "    device: torch.device,\n",
        "    *,\n",
        "    save_dir: Optional[str] = None,\n",
        ") -> Tuple[torch.Tensor, Dict[str, object]]:\n",
        "    \"\"\"Detect, standardize, and normalize an RGB image located on disk.\"\"\"\n",
        "\n",
        "    img = load_image(image_path)\n",
        "    face_rgb, metadata = face_processor.process_image(\n",
        "        img,\n",
        "        identifier=image_path,\n",
        "        save_dir=save_dir,\n",
        "    )\n",
        "    img_tensor = _prepare_tensor(face_rgb, transforms, device)\n",
        "    return img_tensor, metadata\n",
        "\n",
        "\n",
        "def preprocess_frame(\n",
        "    frame: np.ndarray,\n",
        "    video_processor,\n",
        "    transforms,\n",
        "    device: torch.device,\n",
        "    *,\n",
        "    frame_index: int,\n",
        "    save_dir: Optional[str] = None,\n",
        ") -> Tuple[torch.Tensor, Dict[str, object]]:\n",
        "    \"\"\"Process a single BGR video frame through detection and normalization.\"\"\"\n",
        "\n",
        "    face_rgb, metadata = video_processor.process_frame(\n",
        "        frame,\n",
        "        frame_index,\n",
        "        save_dir=save_dir,\n",
        "    )\n",
        "    img_tensor = _prepare_tensor(face_rgb, transforms, device)\n",
        "    return img_tensor, metadata\n",
        "\n",
        "\n",
        "def _forward_model(model: torch.nn.Module, inputs: torch.Tensor):\n",
        "    outputs = model(inputs)\n",
        "    if isinstance(outputs, list):\n",
        "        outputs = outputs[0]\n",
        "\n",
        "    hm_outputs = outputs[\"hm\"]\n",
        "    cls_outputs = outputs[\"cls\"]\n",
        "    hm_preds = _sigmoid(hm_outputs).cpu().numpy()\n",
        "    cls_preds = cls_outputs.detach().cpu().numpy()\n",
        "    return hm_preds, cls_preds\n",
        "\n",
        "\n",
        "def _collect_media_paths(\n",
        "    data_dir: str,\n",
        ") -> Tuple[List[Tuple[str, str]], List[str], List[str]]:\n",
        "    media_entries: List[Tuple[str, str]] = []\n",
        "    image_paths: List[str] = []\n",
        "    video_paths: List[str] = []\n",
        "\n",
        "    for root, _, files in os.walk(data_dir):\n",
        "        for fname in files:\n",
        "            ext = os.path.splitext(fname)[1].lower()\n",
        "            full_path = os.path.join(root, fname)\n",
        "            if ext in IMAGE_EXTENSIONS:\n",
        "                image_paths.append(full_path)\n",
        "                media_entries.append((\"image\", full_path))\n",
        "            elif ext in VIDEO_EXTENSIONS:\n",
        "                video_paths.append(full_path)\n",
        "\n",
        "                media_entries.append((\"video\", full_path))\n",
        "\n",
        "    return media_entries, image_paths, video_paths\n",
        "\n",
        "\n",
        "def run_inference():\n",
        "    \"\"\"Load checkpoints, preprocess images, and print prediction metadata.\"\"\"\n",
        "\n",
        "    args = parse_args()\n",
        "\n",
        "    cfg, model, transforms, device, image_size = prepare_model(\n",
        "        args.config, args.weights\n",
        "    )\n",
        "\n",
        "    target_size = int(max(image_size))\n",
        "    face_config = FacePreprocessingConfig(\n",
        "        target_size=target_size,\n",
        "        align_size=target_size,\n",
        "    )\n",
        "    face_processor = FacePreprocessor(args.yunet_path, face_config)\n",
        "    save_dir = args.save_preprocessed_images_path\n",
        "\n",
        "    data_dir = args.data_dir\n",
        "    if not os.path.isdir(data_dir):\n",
        "        raise FileNotFoundError(f\"Could not find data directory: {data_dir}\")\n",
        "\n",
        "    media_entries, image_paths, video_paths = _collect_media_paths(data_dir)\n",
        "\n",
        "    if not media_entries:\n",
        "        raise FileNotFoundError(\n",
        "            f\"No compatible media found in '{data_dir}'. Supported image extensions: \"\n",
        "            f\"{', '.join(IMAGE_EXTENSIONS)}. Supported video extensions: \"\n",
        "            f\"{', '.join(VIDEO_EXTENSIONS)}.\"\n",
        "        )\n",
        "\n",
        "    if image_paths:\n",
        "        print(f\"Loaded {len(image_paths)} images from {data_dir}/\")\n",
        "    if video_paths:\n",
        "        print(f\"Loaded {len(video_paths)} videos from {data_dir}/\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    if args.skip_fixed_crop:\n",
        "        print(\n",
        "            \"--skip-fixed-crop is ignored; adaptive face preprocessing is always applied.\")\n",
        "\n",
        "    predictions: List[Tuple[str, int]] = []\n",
        "\n",
        "    for media_type, media_path in media_entries:\n",
        "        if media_type == \"image\":\n",
        "            image_path = media_path\n",
        "            with torch.no_grad():\n",
        "                start_time = time.time()\n",
        "                inputs, face_metadata = preprocess_image(\n",
        "                    image_path,\n",
        "                    face_processor,\n",
        "                    transforms,\n",
        "                    device,\n",
        "                    save_dir=save_dir,\n",
        "                )\n",
        "                hm_preds, cls_preds = _forward_model(model, inputs)\n",
        "                score = float(cls_preds[0][-1])\n",
        "                label_int = 1 if score > cfg.TEST.threshold else 0\n",
        "                label_text = \"Fake\" if label_int else \"Real\"\n",
        "                elapsed = time.time() - start_time\n",
        "\n",
        "            predictions.append((os.path.basename(image_path), label_int))\n",
        "\n",
        "            if args.verbose:\n",
        "                print(\"----------------------------------------\")\n",
        "                print(f\"Image: {os.path.basename(image_path)}\")\n",
        "                print(f\"Prediction: {label_text}\")\n",
        "                print(f\"Fake score: {score:.4f}\")\n",
        "                print(f\"Heatmap max value: {hm_preds.max():.4f}\")\n",
        "                print(f\"Inference time: {elapsed:.3f}s\")\n",
        "                print(f\"Face detected: {face_metadata['face_detected']}\")\n",
        "                if face_metadata[\"face_detected\"]:\n",
        "                    det_score = face_metadata.get(\"detection_score\")\n",
        "                    if det_score is not None:\n",
        "                        print(\n",
        "                            f\"Detector: {face_metadata['detector']} (score={det_score:.3f})\"\n",
        "                        )\n",
        "                    else:\n",
        "                        print(f\"Detector: {face_metadata['detector']}\")\n",
        "                    fallback_stage = face_metadata.get(\"fallback\")\n",
        "                    if fallback_stage and fallback_stage != \"full_frame_letterbox\":\n",
        "                        print(f\"Fallback stage: {fallback_stage}\")\n",
        "                else:\n",
        "                    print(\n",
        "                        f\"Fallback pipeline: {face_metadata.get('fallback')} (reason={face_metadata.get('detection_reason')})\"\n",
        "                    )\n",
        "                if face_metadata.get(\"output_path\"):\n",
        "                    print(f\"Saved preprocess: {face_metadata['output_path']}\")\n",
        "\n",
        "        elif media_type == \"video\":\n",
        "            video_path = media_path\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            if not cap.isOpened():\n",
        "                print(\"----------------------------------------\")\n",
        "                print(f\"Video: {os.path.basename(video_path)}\")\n",
        "                print(\"Error: unable to open video file.\")\n",
        "                cap.release()\n",
        "                continue\n",
        "\n",
        "            video_id = os.path.splitext(os.path.basename(video_path))[0]\n",
        "            video_processor = face_processor.create_video_processor(video_id)\n",
        "            frame_scores: List[np.ndarray] = []\n",
        "            heatmap_maxes: List[float] = []\n",
        "            frame_count = 0\n",
        "            processed_frames = 0\n",
        "            start_time = time.time()\n",
        "            frame_metadata: List[Dict[str, object]] = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                while True:\n",
        "                    ret, frame = cap.read()\n",
        "                    if not ret:\n",
        "                        break\n",
        "                    frame_count += 1\n",
        "                    if frame_count % args.video_frame_step:\n",
        "                        continue\n",
        "\n",
        "                    inputs, face_meta = preprocess_frame(\n",
        "                        frame,\n",
        "                        video_processor,\n",
        "                        transforms,\n",
        "                        device,\n",
        "                        frame_index=frame_count - 1,\n",
        "                        save_dir=save_dir,\n",
        "                    )\n",
        "                    hm_preds, cls_preds = _forward_model(model, inputs)\n",
        "                    heatmap_maxes.append(float(hm_preds.max()))\n",
        "                    frame_scores.append(cls_preds[0])\n",
        "                    processed_frames += 1\n",
        "                    frame_metadata.append(face_meta)\n",
        "\n",
        "            cap.release()\n",
        "\n",
        "            if not frame_scores:\n",
        "                print(\"----------------------------------------\")\n",
        "                print(f\"Video: {os.path.basename(video_path)}\")\n",
        "                print(\n",
        "                    \"Error: no frames were processed. Check --video-frame-step or file integrity.\")\n",
        "                continue\n",
        "\n",
        "            mean_logits = np.mean(frame_scores, axis=0)\n",
        "            score = float(mean_logits[-1])\n",
        "            label_int = 1 if score > cfg.TEST.threshold else 0\n",
        "            label_text = \"Fake\" if label_int else \"Real\"\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            predictions.append((os.path.basename(video_path), label_int))\n",
        "\n",
        "            if args.verbose:\n",
        "                print(\"----------------------------------------\")\n",
        "                print(f\"Video: {os.path.basename(video_path)}\")\n",
        "                print(f\"Prediction: {label_text}\")\n",
        "                print(f\"Fake score (mean logits): {score:.4f}\")\n",
        "                print(f\"Frames decoded: {frame_count}\")\n",
        "                print(f\"Frames evaluated: {processed_frames}\")\n",
        "                print(\n",
        "                    f\"Heatmap max value (max over frames): {max(heatmap_maxes):.4f}\")\n",
        "                print(f\"Inference time: {elapsed:.3f}s\")\n",
        "                if frame_metadata:\n",
        "                    successes = sum(\n",
        "                        1 for meta in frame_metadata if meta.get(\"face_detected\"))\n",
        "                    tracker_frames = sum(\n",
        "                        1\n",
        "                        for meta in frame_metadata\n",
        "                        if meta.get(\"temporal_source\") == \"tracker\"\n",
        "                    )\n",
        "                    fallback_frames = sum(\n",
        "                        1\n",
        "                        for meta in frame_metadata\n",
        "                        if meta.get(\"fallback\") == \"full_frame_letterbox\"\n",
        "                        or meta.get(\"temporal_source\") == \"fallback\"\n",
        "                    )\n",
        "                    print(\n",
        "                        f\"Face detection success frames: {successes}/{len(frame_metadata)}\")\n",
        "                    if tracker_frames:\n",
        "                        print(f\"Tracker-rescued frames: {tracker_frames}\")\n",
        "                    if fallback_frames:\n",
        "                        print(f\"Letterbox fallback frames: {fallback_frames}\")\n",
        "\n",
        "    submission_path = os.path.join(os.getcwd(), \"submission.csv\")\n",
        "    with open(submission_path, \"w\", newline=\"\") as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow([\"filename\", \"label\"])\n",
        "        for filename, label in predictions:\n",
        "            writer.writerow([filename, label])\n",
        "\n",
        "    print(f\"Saved {len(predictions)} predictions to {submission_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "17275618",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "file : task\n",
            "jupyter notebook\n",
            "제출 완료\n"
          ]
        }
      ],
      "source": [
        "import aifactory.score as aif\n",
        "\n",
        "aif.submit(model_name=\"laa_net\",\n",
        "           key=\"d931bf47-e65f-4f38-a3c7-8b109102f7d5\"\n",
        "           )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deepfake-detection",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
